{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa6af4d3",
      "metadata": {
        "id": "aa6af4d3"
      },
      "source": [
        "# Using PyTorch Lightning with Tune\n",
        "\n",
        "(tune-vanilla-pytorch-lightning-ref)=\n",
        "\n",
        "PyTorch Lightning is a framework which brings structure into training PyTorch models. It\n",
        "aims to avoid boilerplate code, so you don't have to write the same training\n",
        "loops all over again when building a new model.\n",
        "\n",
        "```{image} /images/pytorch_lightning_full.png\n",
        ":align: center\n",
        "```\n",
        "\n",
        "The main abstraction of PyTorch Lightning is the `LightningModule` class, which\n",
        "should be extended by your application. There is [a great post on how to transfer your models from vanilla PyTorch to Lightning](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09).\n",
        "\n",
        "The class structure of PyTorch Lightning makes it very easy to define and tune model\n",
        "parameters. This tutorial will show you how to use Tune to find the best set of\n",
        "parameters for your application on the example of training a MNIST classifier. Notably,\n",
        "the `LightningModule` does not have to be altered at all for this - so you can\n",
        "use it plug and play for your existing models, assuming their parameters are configurable!\n",
        "\n",
        ":::{note}\n",
        "To run this example, you will need to install the following:\n",
        "\n",
        "```bash\n",
        "$ pip install \"ray[tune]\" torch torchvision pytorch-lightning\n",
        "```\n",
        ":::\n",
        "\n",
        "```{contents}\n",
        ":backlinks: none\n",
        ":local: true\n",
        "```\n",
        "\n",
        "## PyTorch Lightning classifier for MNIST\n",
        "\n",
        "Let's first start with the basic PyTorch Lightning implementation of an MNIST classifier.\n",
        "This classifier does not include any tuning code at this point.\n",
        "\n",
        "Our example builds on the MNIST example from the [blog post we talked about\n",
        "earlier](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09).\n",
        "\n",
        "First, we run some imports:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"ray[tune]\" torch torchvision pytorch-lightning"
      ],
      "metadata": {
        "id": "Xp7LhZRSGgn7",
        "outputId": "e474c8d3-08d2-4695-fc7b-4c37eed01183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Xp7LhZRSGgn7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray[tune]\n",
            "  Downloading ray-2.3.1-cp39-cp39-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.1.post0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (3.11.0)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (1.53.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (6.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (1.0.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (4.3.3)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (1.22.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (22.2.0)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (0.8.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from ray[tune]) (1.5.3)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m556.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Collecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray[tune]) (3.2.0)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray[tune]) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->ray[tune]) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ray[tune]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ray[tune]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ray[tune]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->ray[tune]) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: distlib, virtualenv, tensorboardX, multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, ray, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 distlib-0.3.6 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.1.post0 ray-2.3.1 tensorboardX-2.6 torchmetrics-0.11.4 virtualenv-20.21.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e6e77570",
      "metadata": {
        "id": "e6e77570"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c442e73",
      "metadata": {
        "id": "3c442e73"
      },
      "source": [
        "And then there is the Lightning model adapted from the blog post.\n",
        "Note that we left out the test set validation and made the model parameters\n",
        "configurable through a `config` dict that is passed on initialization.\n",
        "Also, we specify a `data_dir` where the MNIST data will be stored. Note that\n",
        "we use a `FileLock` for downloading data so that the dataset is only downloaded\n",
        "once per node.\n",
        "Lastly, we added a new metric, the validation accuracy, to the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "48b20f48",
      "metadata": {
        "id": "48b20f48"
      },
      "outputs": [],
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    This has been adapted from\n",
        "    https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data_dir=None):\n",
        "        super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "        self.data_dir = data_dir or os.getcwd()\n",
        "\n",
        "        self.layer_1_size = config[\"layer_1_size\"]\n",
        "        self.layer_2_size = config[\"layer_2_size\"]\n",
        "        self.lr = config[\"lr\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "\n",
        "        # mnist images are (1, 28, 28) (channels, width, height)\n",
        "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
        "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
        "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, width, height = x.size()\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = self.layer_1(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def accuracy(self, logits, labels):\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        accuracy = correct / len(labels)\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "\n",
        "        self.log(\"ptl/train_loss\", loss)\n",
        "        self.log(\"ptl/train_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    @staticmethod\n",
        "    def download_data(data_dir):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
        "        ])\n",
        "        with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
        "            return MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        mnist_train = self.download_data(self.data_dir)\n",
        "\n",
        "        self.mnist_train, self.mnist_val = random_split(\n",
        "            mnist_train, [55000, 5000])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def train_mnist(config):\n",
        "    model = LightningMNISTClassifier(config)\n",
        "    trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False)\n",
        "\n",
        "    trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da1c3632",
      "metadata": {
        "id": "da1c3632"
      },
      "source": [
        "And that's it! You can now run `train_mnist(config)` to train the classifier, e.g.\n",
        "like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "86df3d39",
      "metadata": {
        "id": "86df3d39"
      },
      "outputs": [],
      "source": [
        "def train_mnist_no_tune():\n",
        "    config = {\n",
        "        \"layer_1_size\": 128,\n",
        "        \"layer_2_size\": 256,\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64\n",
        "    }\n",
        "    train_mnist(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edcc0991",
      "metadata": {
        "id": "edcc0991"
      },
      "source": [
        "## Tuning the model parameters\n",
        "\n",
        "The parameters above should give you a good accuracy of over 90% already. However,\n",
        "we might improve on this simply by changing some of the hyperparameters. For instance,\n",
        "maybe we get an even higher accuracy if we used a larger batch size.\n",
        "\n",
        "Instead of guessing the parameter values, let's use Tune to systematically try out\n",
        "parameter combinations and find the best performing set.\n",
        "\n",
        "First, we need some additional imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "34faeb3b",
      "metadata": {
        "id": "34faeb3b"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# import setproctitle\n",
        "from ray import tune, air\n",
        "from ray.air import session\n",
        "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
        "    TuneReportCheckpointCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65b9c5f",
      "metadata": {
        "id": "f65b9c5f"
      },
      "source": [
        "### Talking to Tune with a PyTorch Lightning callback\n",
        "\n",
        "PyTorch Lightning introduced [Callbacks](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html)\n",
        "that can be used to plug custom functions into the training loop. This way the original\n",
        "`LightningModule` does not have to be altered at all. Also, we could use the same\n",
        "callback for multiple modules.\n",
        "\n",
        "Ray Tune comes with ready-to-use PyTorch Lightning callbacks. To report metrics\n",
        "back to Tune after each validation epoch, we will use the `TuneReportCallback`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4bab80bc",
      "metadata": {
        "id": "4bab80bc",
        "outputId": "4f0b760a-0f66-4a59-c815-27c5cc92fe01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.integration.pytorch_lightning.TuneReportCallback at 0x7fbfcef2c790>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "TuneReportCallback(\n",
        "    {\n",
        "        \"loss\": \"ptl/val_loss\",\n",
        "        \"mean_accuracy\": \"ptl/val_accuracy\"\n",
        "    },\n",
        "    on=\"validation_end\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286a1070",
      "metadata": {
        "id": "286a1070"
      },
      "source": [
        "This callback will take the `val_loss` and `val_accuracy` values\n",
        "from the PyTorch Lightning trainer and report them to Tune as the `loss`\n",
        "and `mean_accuracy`, respectively.\n",
        "\n",
        "### Adding the Tune training function\n",
        "\n",
        "Then we specify our training function. Note that we added the `data_dir` as a\n",
        "parameter here to avoid\n",
        "that each training run downloads the full MNIST dataset. Instead, we want to access\n",
        "a shared data location.\n",
        "\n",
        "We are also able to specify the number of epochs to train each model, and the number\n",
        "of GPUs we want to use for training. We also create a TensorBoard logger that writes\n",
        "logfiles directly into Tune's root trial directory - if we didn't do that PyTorch\n",
        "Lightning would create subdirectories, and each trial would thus be shown twice in\n",
        "TensorBoard, one time for Tune's logs, and another time for PyTorch Lightning's logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74e7d1c2",
      "metadata": {
        "id": "74e7d1c2"
      },
      "outputs": [],
      "source": [
        "def train_mnist_tune(config, num_epochs=10, num_gpus=0, data_dir=\"~/data\"):\n",
        "    data_dir = os.path.expanduser(data_dir)\n",
        "    model = LightningMNISTClassifier(config, data_dir)\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        logger=TensorBoardLogger(\n",
        "            save_dir=os.getcwd(), name=\"\", version=\".\"),\n",
        "        enable_progress_bar=False,\n",
        "        callbacks=[\n",
        "            TuneReportCallback(\n",
        "                {\n",
        "                    \"loss\": \"ptl/val_loss\",\n",
        "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
        "                },\n",
        "                on=\"validation_end\")\n",
        "        ])\n",
        "    trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0f6d6e",
      "metadata": {
        "id": "cf0f6d6e"
      },
      "source": [
        "### Configuring the search space\n",
        "\n",
        "Now we configure the parameter search space. We would like to choose between three\n",
        "different layer and batch sizes. The learning rate should be sampled uniformly between\n",
        "`0.0001` and `0.1`. The `tune.loguniform()` function is syntactic sugar to make\n",
        "sampling between these different orders of magnitude easier, specifically\n",
        "we are able to also sample small values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a50645e9",
      "metadata": {
        "id": "a50645e9"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
        "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
        "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "    \"batch_size\": tune.choice([32, 64, 128]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fb9ecd",
      "metadata": {
        "id": "b1fb9ecd"
      },
      "source": [
        "### Selecting a scheduler\n",
        "\n",
        "In this example, we use an [Asynchronous Hyperband](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
        "scheduler. This scheduler decides at each iteration which trials are likely to perform\n",
        "badly, and stops these trials. This way we don't waste any resources on bad hyperparameter\n",
        "configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a2596b01",
      "metadata": {
        "id": "a2596b01"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "    max_t=num_epochs,\n",
        "    grace_period=1,\n",
        "    reduction_factor=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a49ae58",
      "metadata": {
        "id": "9a49ae58"
      },
      "source": [
        "### Changing the CLI output\n",
        "\n",
        "We instantiate a `CLIReporter` to specify which metrics we would like to see in our\n",
        "output tables in the command line. This is optional, but can be used to make sure our\n",
        "output tables only include information we would like to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cd605a16",
      "metadata": {
        "id": "cd605a16"
      },
      "outputs": [],
      "source": [
        "reporter = CLIReporter(\n",
        "    parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
        "    metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec9a305",
      "metadata": {
        "id": "5ec9a305"
      },
      "source": [
        "### Passing constants to the train function\n",
        "\n",
        "The `data_dir`, `num_epochs` and `num_gpus` we pass to the training function\n",
        "are constants. To avoid including them as non-configurable parameters in the `config`\n",
        "specification, we can use `tune.with_parameters` to wrap around the training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "332668dc",
      "metadata": {
        "id": "332668dc"
      },
      "outputs": [],
      "source": [
        "gpus_per_trial = 0\n",
        "data_dir = \"~/data\"\n",
        "\n",
        "train_fn_with_parameters = tune.with_parameters(train_mnist_tune,\n",
        "                                                num_epochs=num_epochs,\n",
        "                                                num_gpus=gpus_per_trial,\n",
        "                                                data_dir=data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feef8c39",
      "metadata": {
        "id": "feef8c39"
      },
      "source": [
        "### Training with GPUs\n",
        "\n",
        "We can specify how many resources Tune should request for each trial.\n",
        "This also includes GPUs.\n",
        "\n",
        "PyTorch Lightning takes care of moving the training to the GPUs. We\n",
        "already made sure that our code is compatible with that, so there's\n",
        "nothing more to do here other than to specify the number of GPUs\n",
        "we would like to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dc402716",
      "metadata": {
        "id": "dc402716"
      },
      "outputs": [],
      "source": [
        "resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca050dfa",
      "metadata": {
        "id": "ca050dfa"
      },
      "source": [
        "You can also specify {ref}`fractional GPUs for Tune <tune-parallelism>`,\n",
        "allowing multiple trials to share GPUs and thus increase concurrency under resource constraints.\n",
        "While the `gpus_per_trial` passed into\n",
        "Tune is a decimal value, the `gpus` passed into the `pl.Trainer` should still be an integer.\n",
        "Please note that if using fractional GPUs, it is the user's responsibility to\n",
        "make sure multiple trials can share GPUs and there is enough memory to do so.\n",
        "Ray does not automatically handle this for you.\n",
        "\n",
        "If you want to use multiple GPUs per trial, you should check out the\n",
        "[Ray Lightning Library](https://github.com/ray-project/ray_lightning).\n",
        "This library makes it easy to run multiple concurrent trials with Ray Tune, with each trial also running\n",
        "in a distributed fashion using Ray.\n",
        "\n",
        "### Putting it together\n",
        "\n",
        "Lastly, we need to create a `Tuner()` object and start Ray Tune with `tuner.fit()`.\n",
        "\n",
        "The full code looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ea182330",
      "metadata": {
        "id": "ea182330"
      },
      "outputs": [],
      "source": [
        "def tune_mnist_asha(num_samples=10, num_epochs=10, gpus_per_trial=0, data_dir=\"~/data\"):\n",
        "    config = {\n",
        "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
        "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([32, 64, 128]),\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    # reporter = CLIReporter(\n",
        "    reporter = JupyterNotebookReporter(\n",
        "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
        "\n",
        "    train_fn_with_parameters = tune.with_parameters(train_mnist_tune,\n",
        "                                                    num_epochs=num_epochs,\n",
        "                                                    num_gpus=gpus_per_trial,\n",
        "                                                    data_dir=data_dir)\n",
        "    resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n",
        "    \n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(\n",
        "            train_fn_with_parameters,\n",
        "            resources=resources_per_trial\n",
        "        ),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            local_dir=\"./runs\",\n",
        "            name=\"tune_mnist_asha\",\n",
        "            progress_reporter=reporter,\n",
        "            log_to_file=True,\n",
        "        ),\n",
        "        param_space=config,\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", results.get_best_result().config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb96b6c",
      "metadata": {
        "id": "1fb96b6c"
      },
      "source": [
        "In the example above, Tune runs 10 trials with different hyperparameter configurations.\n",
        "An example output could look like so:\n",
        "\n",
        "```{code-block} bash\n",
        ":emphasize-lines: 12\n",
        "\n",
        "  +------------------------------+------------+-------+----------------+----------------+-------------+--------------+----------+-----------------+----------------------+\n",
        "  | Trial name                   | status     | loc   |   layer_1_size |   layer_2_size |          lr |   batch_size |     loss |   mean_accuracy |   training_iteration |\n",
        "  |------------------------------+------------+-------+----------------+----------------+-------------+--------------+----------+-----------------+----------------------|\n",
        "  | train_mnist_tune_63ecc_00000 | TERMINATED |       |            128 |             64 | 0.00121197  |          128 | 0.120173 |       0.972461  |                   10 |\n",
        "  | train_mnist_tune_63ecc_00001 | TERMINATED |       |             64 |            128 | 0.0301395   |          128 | 0.454836 |       0.868164  |                    4 |\n",
        "  | train_mnist_tune_63ecc_00002 | TERMINATED |       |             64 |            128 | 0.0432097   |          128 | 0.718396 |       0.718359  |                    1 |\n",
        "  | train_mnist_tune_63ecc_00003 | TERMINATED |       |             32 |            128 | 0.000294669 |           32 | 0.111475 |       0.965764  |                   10 |\n",
        "  | train_mnist_tune_63ecc_00004 | TERMINATED |       |             32 |            256 | 0.000386664 |           64 | 0.133538 |       0.960839  |                    8 |\n",
        "  | train_mnist_tune_63ecc_00005 | TERMINATED |       |            128 |            128 | 0.0837395   |           32 | 2.32628  |       0.0991242 |                    1 |\n",
        "  | train_mnist_tune_63ecc_00006 | TERMINATED |       |             64 |            128 | 0.000158761 |          128 | 0.134595 |       0.959766  |                   10 |\n",
        "  | train_mnist_tune_63ecc_00007 | TERMINATED |       |             64 |             64 | 0.000672126 |           64 | 0.118182 |       0.972903  |                   10 |\n",
        "  | train_mnist_tune_63ecc_00008 | TERMINATED |       |            128 |             64 | 0.000502428 |           32 | 0.11082  |       0.975518  |                   10 |\n",
        "  | train_mnist_tune_63ecc_00009 | TERMINATED |       |             64 |            256 | 0.00112894  |           32 | 0.13472  |       0.971935  |                    8 |\n",
        "  +------------------------------+------------+-------+----------------+----------------+-------------+--------------+----------+-----------------+----------------------+\n",
        "```\n",
        "\n",
        "As you can see in the `training_iteration` column, trials with a high loss\n",
        "(and low accuracy) have been terminated early. The best performing trial used\n",
        "`layer_1_size=128`, `layer_2_size=64`, `lr=0.000502428` and\n",
        "`batch_size=32`.\n",
        "\n",
        "## Using Population Based Training to find the best parameters\n",
        "\n",
        "The `ASHAScheduler` terminates those trials early that show bad performance.\n",
        "Sometimes, this stops trials that would get better after more training steps,\n",
        "and which might eventually even show better performance than other configurations.\n",
        "\n",
        "Another popular method for hyperparameter tuning, called\n",
        "[Population Based Training](https://deepmind.com/blog/article/population-based-training-neural-networks),\n",
        "instead perturbs hyperparameters during the training run. Tune implements PBT, and\n",
        "we only need to make some slight adjustments to our code.\n",
        "\n",
        "### Adding checkpoints to the PyTorch Lightning module\n",
        "\n",
        "First, we need to introduce\n",
        "another callback to save model checkpoints. Since Tune requires a call to\n",
        "`session.report()` after creating a new checkpoint to register it, we will use\n",
        "a combined reporting and checkpointing callback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f86e4d8",
      "metadata": {
        "id": "7f86e4d8",
        "outputId": "ab9db29a-f6dc-47d8-9fc1-42cc3d9cbf6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback at 0x7fbfcef2c700>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "TuneReportCheckpointCallback(\n",
        "    metrics={\n",
        "        \"loss\": \"ptl/val_loss\",\n",
        "        \"mean_accuracy\": \"ptl/val_accuracy\"\n",
        "    },\n",
        "    filename=\"checkpoint\",\n",
        "    on=\"validation_end\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a76d5b",
      "metadata": {
        "id": "33a76d5b"
      },
      "source": [
        "The `checkpoint` value is the name of the checkpoint file within the\n",
        "checkpoint directory.\n",
        "\n",
        "We also include checkpoint loading in our training function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "746e962a",
      "metadata": {
        "id": "746e962a"
      },
      "outputs": [],
      "source": [
        "def train_mnist_tune_checkpoint(config,\n",
        "                                checkpoint_dir=None,\n",
        "                                num_epochs=10,\n",
        "                                num_gpus=0,\n",
        "                                data_dir=\"~/data\"):\n",
        "    data_dir = os.path.expanduser(data_dir)\n",
        "    kwargs = {\n",
        "        \"max_epochs\": num_epochs,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        \"gpus\": math.ceil(num_gpus),\n",
        "        \"logger\": TensorBoardLogger(\n",
        "            save_dir=os.getcwd(), name=\"\", version=\".\"),\n",
        "        \"enable_progress_bar\": False,\n",
        "        \"callbacks\": [\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    \"loss\": \"ptl/val_loss\",\n",
        "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
        "                },\n",
        "                filename=\"checkpoint\",\n",
        "                on=\"validation_end\")\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        kwargs[\"resume_from_checkpoint\"] = os.path.join(\n",
        "            checkpoint_dir, \"checkpoint\")\n",
        "\n",
        "    model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n",
        "    trainer = pl.Trainer(**kwargs)\n",
        "\n",
        "    trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dc7b46",
      "metadata": {
        "id": "39dc7b46"
      },
      "source": [
        "### Configuring and running Population Based Training\n",
        "\n",
        "We need to call Tune slightly differently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e12a1bd5",
      "metadata": {
        "id": "e12a1bd5"
      },
      "outputs": [],
      "source": [
        "def tune_mnist_pbt(num_samples=10, num_epochs=10, gpus_per_trial=0, data_dir=\"~/data\"):\n",
        "    config = {\n",
        "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
        "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "            \"batch_size\": [32, 64, 128]\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
        "    \n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(\n",
        "            tune.with_parameters(\n",
        "                train_mnist_tune_checkpoint,\n",
        "                num_epochs=num_epochs,\n",
        "                num_gpus=gpus_per_trial,\n",
        "                data_dir=data_dir),\n",
        "            resources={\n",
        "                \"cpu\": 1,\n",
        "                \"gpu\": gpus_per_trial\n",
        "            }\n",
        "        ),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            name=\"tune_mnist_asha\",\n",
        "            progress_reporter=reporter,\n",
        "        ),\n",
        "        param_space=config,\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", results.get_best_result().config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6087f807",
      "metadata": {
        "id": "6087f807"
      },
      "source": [
        "Instead of passing tune parameters to the `config` dict, we start\n",
        "with fixed values, though we are also able to sample some of them, like the\n",
        "layer sizes. Additionally, we have to tell PBT how to perturb the hyperparameters.\n",
        "Note that the layer sizes are not tuned right here. This is because we cannot simply\n",
        "change layer sizes during a training run - which is what would happen in PBT.\n",
        "\n",
        "To test running both of our main scripts (`tune_mnist_asha` and `tune_mnist_pbt`), all you have to do is specify\n",
        "a `data_dir` folder and run the scripts with reasonable parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "eb9faf3e",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eb9faf3e",
        "outputId": "e0b9d619-7efc-4664-f0a9-5431e4b30cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-04-15 08:59:04</td></tr>\n",
              "<tr><td>Running for: </td><td>00:01:16.23        </td></tr>\n",
              "<tr><td>Memory:      </td><td>2.0/12.7 GiB       </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "<div class=\"messages\">\n",
              "  <h3>Messages</h3>\n",
              "  \n",
              "  \n",
              "  Number of errored trials: 16<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_tune_9864d_00000</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00000_0_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0023_2023-04-15_08-57-48/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00001</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00001_1_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0294_2023-04-15_08-57-52/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00002</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00002_2_batch_size=32,layer_1_size=64,layer_2_size=128,lr=0.0002_2023-04-15_08-57-56/error.txt  </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00003</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00003_3_batch_size=32,layer_1_size=64,layer_2_size=64,lr=0.0009_2023-04-15_08-58-02/error.txt   </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00004</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00004_4_batch_size=128,layer_1_size=128,layer_2_size=256,lr=0.0190_2023-04-15_08-58-06/error.txt</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00005</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00005_5_batch_size=32,layer_1_size=32,layer_2_size=256,lr=0.0078_2023-04-15_08-58-10/error.txt  </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00006</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00006_6_batch_size=128,layer_1_size=32,layer_2_size=128,lr=0.0777_2023-04-15_08-58-16/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00007</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00007_7_batch_size=32,layer_1_size=128,layer_2_size=64,lr=0.0002_2023-04-15_08-58-21/error.txt  </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00008</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00008_8_batch_size=64,layer_1_size=64,layer_2_size=128,lr=0.0111_2023-04-15_08-58-25/error.txt  </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00009</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00009_9_batch_size=32,layer_1_size=64,layer_2_size=256,lr=0.0001_2023-04-15_08-58-30/error.txt  </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00010</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00010_10_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0011_2023-04-15_08-58-35/error.txt</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00011</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00011_11_batch_size=128,layer_1_size=32,layer_2_size=64,lr=0.0075_2023-04-15_08-58-39/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00012</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00012_12_batch_size=32,layer_1_size=64,layer_2_size=256,lr=0.0077_2023-04-15_08-58-44/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00013</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00013_13_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0028_2023-04-15_08-58-51/error.txt</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00014</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00014_14_batch_size=64,layer_1_size=128,layer_2_size=64,lr=0.0088_2023-04-15_08-58-55/error.txt </td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00015</td><td style=\"text-align: right;\">           1</td><td>/content/runs/tune_mnist_asha/train_mnist_tune_9864d_00015_15_batch_size=128,layer_1_size=64,layer_2_size=128,lr=0.0110_2023-04-15_08-58-59/error.txt</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".messages {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  padding-left: 1em;\n",
              "  overflow-y: auto;\n",
              "}\n",
              ".messages h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n",
              "\n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  layer_1_size</th><th style=\"text-align: right;\">  layer_2_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  batch_size</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_tune_9864d_00000</td><td>ERROR   </td><td>172.28.0.12:1305</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.00227975 </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00001</td><td>ERROR   </td><td>172.28.0.12:1373</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.0294497  </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00002</td><td>ERROR   </td><td>172.28.0.12:1431</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.000167116</td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00003</td><td>ERROR   </td><td>172.28.0.12:1492</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.000860193</td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00004</td><td>ERROR   </td><td>172.28.0.12:1549</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.0189798  </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00005</td><td>ERROR   </td><td>172.28.0.12:1606</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.00784574 </td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00006</td><td>ERROR   </td><td>172.28.0.12:1672</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.0776658  </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00007</td><td>ERROR   </td><td>172.28.0.12:1733</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.000206985</td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00008</td><td>ERROR   </td><td>172.28.0.12:1792</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.0111395  </td><td style=\"text-align: right;\">          64</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00009</td><td>ERROR   </td><td>172.28.0.12:1851</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.000146388</td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00010</td><td>ERROR   </td><td>172.28.0.12:1914</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00109926 </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00011</td><td>ERROR   </td><td>172.28.0.12:1969</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00747598 </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00012</td><td>ERROR   </td><td>172.28.0.12:2029</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.00772963 </td><td style=\"text-align: right;\">          32</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00013</td><td>ERROR   </td><td>172.28.0.12:2100</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.00275954 </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00014</td><td>ERROR   </td><td>172.28.0.12:2158</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00876317 </td><td style=\"text-align: right;\">          64</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00015</td><td>ERROR   </td><td>172.28.0.12:2213</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.0109957  </td><td style=\"text-align: right;\">         128</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:57:47,161\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "2023-04-15 08:57:52,524\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00000: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1305, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>date               </th><th>experiment_id                   </th><th>hostname    </th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_tune_9864d_00000</td><td>2023-04-15_08-57-52</td><td>de1bc1ae8b64421e9638eaf6f9f7cfc2</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1305</td><td style=\"text-align: right;\"> 1681549072</td><td>9864d_00000</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00001</td><td>2023-04-15_08-57-56</td><td>e0d9cf9cfc794dc086e11eb7fe6a89d7</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1373</td><td style=\"text-align: right;\"> 1681549076</td><td>9864d_00001</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00002</td><td>2023-04-15_08-58-02</td><td>c183f4a07b0445918798b01335088b04</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1431</td><td style=\"text-align: right;\"> 1681549082</td><td>9864d_00002</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00003</td><td>2023-04-15_08-58-06</td><td>41b71bded15649d79c9dd2fa57c1e0c7</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1492</td><td style=\"text-align: right;\"> 1681549086</td><td>9864d_00003</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00004</td><td>2023-04-15_08-58-10</td><td>39b5770cd7224468b54e71d6925d8167</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1549</td><td style=\"text-align: right;\"> 1681549090</td><td>9864d_00004</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00005</td><td>2023-04-15_08-58-15</td><td>29d22421f8714ff691d233c25918e5eb</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1606</td><td style=\"text-align: right;\"> 1681549095</td><td>9864d_00005</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00006</td><td>2023-04-15_08-58-20</td><td>448a1514d3b7401eab98eb35e0170d21</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1672</td><td style=\"text-align: right;\"> 1681549100</td><td>9864d_00006</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00007</td><td>2023-04-15_08-58-25</td><td>ffadebd27e104b639589a6527ca90ac7</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1733</td><td style=\"text-align: right;\"> 1681549105</td><td>9864d_00007</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00008</td><td>2023-04-15_08-58-30</td><td>4225d2058eb745ff8f0ec369dcb9b972</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1792</td><td style=\"text-align: right;\"> 1681549110</td><td>9864d_00008</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00009</td><td>2023-04-15_08-58-35</td><td>d5003cabf8cb45a281fe6e1bc5f5b6c8</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1851</td><td style=\"text-align: right;\"> 1681549115</td><td>9864d_00009</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00010</td><td>2023-04-15_08-58-39</td><td>d0e7c31b863847db91eb8e740733781c</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1914</td><td style=\"text-align: right;\"> 1681549119</td><td>9864d_00010</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00011</td><td>2023-04-15_08-58-44</td><td>1e9f54843c39455c8af13ceafd8930f8</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 1969</td><td style=\"text-align: right;\"> 1681549124</td><td>9864d_00011</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00012</td><td>2023-04-15_08-58-51</td><td>537f119b9c354168a453edfd26ff1926</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2029</td><td style=\"text-align: right;\"> 1681549131</td><td>9864d_00012</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00013</td><td>2023-04-15_08-58-55</td><td>1c93ba7053fa4385abd97965ee5fd32b</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2100</td><td style=\"text-align: right;\"> 1681549135</td><td>9864d_00013</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00014</td><td>2023-04-15_08-58-59</td><td>0a110ea9d0674d789fde6413f4ae15ee</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2158</td><td style=\"text-align: right;\"> 1681549139</td><td>9864d_00014</td></tr>\n",
              "<tr><td>train_mnist_tune_9864d_00015</td><td>2023-04-15_08-59-04</td><td>3df2ec16cf6e45abb67fb3d16dd5f527</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2213</td><td style=\"text-align: right;\"> 1681549144</td><td>9864d_00015</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:57:56,471\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00001: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1373, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:02,470\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00002: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1431, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:06,725\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00003: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1492, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:10,618\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00004: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1549, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:16,117\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00005: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1606, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:20,956\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00006: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1672, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:25,603\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00007: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1733, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:30,559\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00008: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1792, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:35,294\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00009: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1851, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:39,600\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00010: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1914, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:44,577\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00011: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=1969, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:51,404\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00012: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2029, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:55,538\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00013: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2100, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:58:59,639\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00014: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2158, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:59:04,697\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_9864d_00015: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2213, ip=172.28.0.12, repr=train_mnist_tune)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 406, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-7-22c5cb64cf8c>\", line 4, in train_mnist_tune\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:59:04,752\tERROR tune.py:794 -- Trials did not complete: [train_mnist_tune_9864d_00000, train_mnist_tune_9864d_00001, train_mnist_tune_9864d_00002, train_mnist_tune_9864d_00003, train_mnist_tune_9864d_00004, train_mnist_tune_9864d_00005, train_mnist_tune_9864d_00006, train_mnist_tune_9864d_00007, train_mnist_tune_9864d_00008, train_mnist_tune_9864d_00009, train_mnist_tune_9864d_00010, train_mnist_tune_9864d_00011, train_mnist_tune_9864d_00012, train_mnist_tune_9864d_00013, train_mnist_tune_9864d_00014, train_mnist_tune_9864d_00015]\n",
            "2023-04-15 08:59:04,754\tINFO tune.py:798 -- Total run time: 76.47 seconds (76.22 seconds for the tuning loop).\n",
            "2023-04-15 08:59:04,764\tWARNING experiment_analysis.py:621 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f96f9654ca19>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtune_mnist_asha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-858d4b916040>\u001b[0m in \u001b[0;36mtune_mnist_asha\u001b[0;34m(num_samples, num_epochs, gpus_per_trial, data_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters found were: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ray/tune/result_grid.py\u001b[0m in \u001b[0;36mget_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
          ]
        }
      ],
      "source": [
        "!mkdir -p data\n",
        "data_dir = \"data/\"\n",
        "\n",
        "tune_mnist_asha(num_samples=16, num_epochs=6, gpus_per_trial=1, data_dir=data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d64c412f",
      "metadata": {
        "id": "d64c412f",
        "outputId": "2afe364f-fb24-4f1e-daba-6e1060db995a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dbd40f87",
      "metadata": {
        "id": "dbd40f87",
        "outputId": "25bddebe-9399-45e0-9854-f84be0494721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py:612: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
            "\n",
            "from ray.air import session\n",
            "\n",
            "def train(config):\n",
            "    # ...\n",
            "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
            "\n",
            "For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html\n",
            "\n",
            "  warnings.warn(\n",
            "2023-04-15 08:59:25,219\tWARNING trial_runner.py:1677 -- You are trying to access _search_alg interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 08:59:25 (running for 00:00:00.10)\n",
            "Memory usage on this node: 1.7/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | RUNNING  | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | PENDING  |                  |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | PENDING  |                  |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | PENDING  |                  |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:59:29,580\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00000: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2363, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>date               </th><th>experiment_id                   </th><th>hostname    </th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00000</td><td>2023-04-15_08-59-29</td><td>94992909a2554833bf07f4d8ef1069c6</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2363</td><td style=\"text-align: right;\"> 1681549169</td><td>d2273_00000</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00001</td><td>2023-04-15_08-59-35</td><td>4dc40b7e31644860ac20428bb1003282</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2425</td><td style=\"text-align: right;\"> 1681549175</td><td>d2273_00001</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00002</td><td>2023-04-15_08-59-39</td><td>fe436bd2956346c29f27647e132c9e1f</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2490</td><td style=\"text-align: right;\"> 1681549179</td><td>d2273_00002</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00003</td><td>2023-04-15_08-59-43</td><td>419f7ac6dad146b5bee5c68fa6fa7d1b</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2548</td><td style=\"text-align: right;\"> 1681549183</td><td>d2273_00003</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00004</td><td>2023-04-15_08-59-48</td><td>9a65e212089248a0b098bbad1ed94b76</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2604</td><td style=\"text-align: right;\"> 1681549188</td><td>d2273_00004</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00005</td><td>2023-04-15_08-59-53</td><td>8a9ace63a26549fd80fb364b328a4929</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2668</td><td style=\"text-align: right;\"> 1681549193</td><td>d2273_00005</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00006</td><td>2023-04-15_08-59-58</td><td>15f799712a614314af0f3fab3502cd1d</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2729</td><td style=\"text-align: right;\"> 1681549198</td><td>d2273_00006</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00007</td><td>2023-04-15_09-00-05</td><td>9b730c4a9ceb42988e84ac7d4a17cc78</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2785</td><td style=\"text-align: right;\"> 1681549205</td><td>d2273_00007</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00008</td><td>2023-04-15_09-00-11</td><td>c5a3c7dbdc814e9194b0742c0e36d1ba</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2854</td><td style=\"text-align: right;\"> 1681549211</td><td>d2273_00008</td></tr>\n",
              "<tr><td>train_mnist_tune_checkpoint_d2273_00009</td><td>2023-04-15_09-00-15</td><td>acd0629373ca40b3b9a0068cbb34b26e</td><td>f806d1e96d68</td><td>172.28.0.12</td><td style=\"text-align: right;\"> 2920</td><td style=\"text-align: right;\"> 1681549215</td><td>d2273_00009</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 08:59:34 (running for 00:00:09.55)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (1 ERROR, 8 PENDING, 1 RUNNING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | RUNNING  | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | PENDING  |                  |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | PENDING  |                  |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | PENDING  |                  |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 1\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:59:35,543\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00001: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2425, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:59:39,715\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00002: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2490, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 08:59:43,641\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00003: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2548, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 08:59:43 (running for 00:00:18.44)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (4 ERROR, 6 PENDING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | PENDING  |                  |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 4\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:59:48,790\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00004: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2604, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 08:59:48 (running for 00:00:23.55)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (4 ERROR, 5 PENDING, 1 RUNNING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | RUNNING  | 172.28.0.12:2604 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | PENDING  |                  |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 4\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:59:53,878\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00005: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2668, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 08:59:53 (running for 00:00:28.67)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (6 ERROR, 4 PENDING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | PENDING  |                  |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | ERROR    | 172.28.0.12:2604 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | ERROR    | 172.28.0.12:2668 |             32 |             64 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 6\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00004_4_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-43/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00005_5_layer_1_size=32,layer_2_size=64_2023-04-15_08-59-49/error.txt  |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 08:59:58,678\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00006: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2729, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 09:00:03 (running for 00:00:38.58)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (7 ERROR, 2 PENDING, 1 RUNNING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | RUNNING  | 172.28.0.12:2785 |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | PENDING  |                  |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | ERROR    | 172.28.0.12:2604 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | ERROR    | 172.28.0.12:2668 |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | ERROR    | 172.28.0.12:2729 |             64 |            256 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 7\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00004_4_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-43/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00005_5_layer_1_size=32,layer_2_size=64_2023-04-15_08-59-49/error.txt  |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00006_6_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-54/error.txt |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 09:00:05,336\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00007: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2785, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 09:00:11,214\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00008: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2854, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 09:00:11 (running for 00:00:45.80)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (8 ERROR, 1 PENDING, 1 RUNNING)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | RUNNING  | 172.28.0.12:2854 |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | PENDING  |                  |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | ERROR    | 172.28.0.12:2604 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | ERROR    | 172.28.0.12:2668 |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | ERROR    | 172.28.0.12:2729 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | ERROR    | 172.28.0.12:2785 |             64 |             64 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 8\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00004_4_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-43/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00005_5_layer_1_size=32,layer_2_size=64_2023-04-15_08-59-49/error.txt  |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00006_6_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-54/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00007_7_layer_1_size=64,layer_2_size=64_2023-04-15_08-59-58/error.txt  |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 09:00:15,622\tERROR trial_runner.py:1062 -- Trial train_mnist_tune_checkpoint_d2273_00009: Error processing event.\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2920, ip=172.28.0.12, repr=train_mnist_tune_checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 337, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/function_trainable.py\", line 654, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/ray/tune/trainable/util.py\", line 398, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-15-90d5fb2b205e>\", line 30, in train_mnist_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: __init__() got an unexpected keyword argument 'gpus'\n",
            "2023-04-15 09:00:15,647\tERROR tune.py:794 -- Trials did not complete: [train_mnist_tune_checkpoint_d2273_00000, train_mnist_tune_checkpoint_d2273_00001, train_mnist_tune_checkpoint_d2273_00002, train_mnist_tune_checkpoint_d2273_00003, train_mnist_tune_checkpoint_d2273_00004, train_mnist_tune_checkpoint_d2273_00005, train_mnist_tune_checkpoint_d2273_00006, train_mnist_tune_checkpoint_d2273_00007, train_mnist_tune_checkpoint_d2273_00008, train_mnist_tune_checkpoint_d2273_00009]\n",
            "2023-04-15 09:00:15,649\tINFO tune.py:798 -- Total run time: 50.45 seconds (50.42 seconds for the tuning loop).\n",
            "2023-04-15 09:00:15,657\tWARNING experiment_analysis.py:621 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-04-15 09:00:15 (running for 00:00:50.42)\n",
            "Memory usage on this node: 2.0/12.7 GiB \n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.7 GiB objects\n",
            "Result logdir: /root/ray_results/tune_mnist_asha\n",
            "Number of trials: 10/10 (10 ERROR)\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "| Trial name                              | status   | loc              |   layer_1_size |   layer_2_size |    lr |   batch_size |\n",
            "|-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 | ERROR    | 172.28.0.12:2363 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 | ERROR    | 172.28.0.12:2425 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 | ERROR    | 172.28.0.12:2490 |             32 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 | ERROR    | 172.28.0.12:2548 |             32 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 | ERROR    | 172.28.0.12:2604 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 | ERROR    | 172.28.0.12:2668 |             32 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 | ERROR    | 172.28.0.12:2729 |             64 |            256 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 | ERROR    | 172.28.0.12:2785 |             64 |             64 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 | ERROR    | 172.28.0.12:2854 |             64 |            128 | 0.001 |           64 |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 | ERROR    | 172.28.0.12:2920 |             64 |             64 | 0.001 |           64 |\n",
            "+-----------------------------------------+----------+------------------+----------------+----------------+-------+--------------+\n",
            "Number of errored trials: 10\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                              |   # failures | error file                                                                                                                                 |\n",
            "|-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_mnist_tune_checkpoint_d2273_00000 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00000_0_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-25/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00001 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00001_1_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-29/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00002 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00002_2_layer_1_size=32,layer_2_size=256_2023-04-15_08-59-35/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00003 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00003_3_layer_1_size=32,layer_2_size=128_2023-04-15_08-59-39/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00004 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00004_4_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-43/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00005 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00005_5_layer_1_size=32,layer_2_size=64_2023-04-15_08-59-49/error.txt  |\n",
            "| train_mnist_tune_checkpoint_d2273_00006 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00006_6_layer_1_size=64,layer_2_size=256_2023-04-15_08-59-54/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00007 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00007_7_layer_1_size=64,layer_2_size=64_2023-04-15_08-59-58/error.txt  |\n",
            "| train_mnist_tune_checkpoint_d2273_00008 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00008_8_layer_1_size=64,layer_2_size=128_2023-04-15_09-00-05/error.txt |\n",
            "| train_mnist_tune_checkpoint_d2273_00009 |            1 | /root/ray_results/tune_mnist_asha/train_mnist_tune_checkpoint_d2273_00009_9_layer_1_size=64,layer_2_size=64_2023-04-15_09-00-11/error.txt  |\n",
            "+-----------------------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7b207c730acb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_mnist_pbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b71bfe2e42d2>\u001b[0m in \u001b[0;36mtune_mnist_pbt\u001b[0;34m(num_samples, num_epochs, gpus_per_trial, data_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters found were: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ray/tune/result_grid.py\u001b[0m in \u001b[0;36mget_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
          ]
        }
      ],
      "source": [
        "tune_mnist_pbt(num_samples=10, num_epochs=6, gpus_per_trial=1, data_dir=data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae0eea6",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3ae0eea6"
      },
      "source": [
        "If you have more resources available (e.g. a GPU), you can modify the above parameters accordingly.\n",
        "\n",
        "An example output of a run could look like this:\n",
        "\n",
        "```bash\n",
        "+-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------+\n",
        "| Trial name                              | status     | loc   |   layer_1_size |   layer_2_size |        lr |   batch_size |      loss |   mean_accuracy |   training_iteration |\n",
        "|-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------|\n",
        "| train_mnist_tune_checkpoint_85489_00000 | TERMINATED |       |            128 |            128 | 0.001     |           64 | 0.108734  |        0.973101 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00001 | TERMINATED |       |            128 |            128 | 0.001     |           64 | 0.093577  |        0.978639 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00002 | TERMINATED |       |            128 |            256 | 0.0008    |           32 | 0.0922348 |        0.979299 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00003 | TERMINATED |       |             64 |            256 | 0.001     |           64 | 0.124648  |        0.973892 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00004 | TERMINATED |       |            128 |             64 | 0.001     |           64 | 0.101717  |        0.975079 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00005 | TERMINATED |       |             64 |             64 | 0.001     |           64 | 0.121467  |        0.969146 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00006 | TERMINATED |       |            128 |            256 | 0.00064   |           32 | 0.053446  |        0.987062 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00007 | TERMINATED |       |            128 |            256 | 0.001     |           64 | 0.129804  |        0.973497 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00008 | TERMINATED |       |             64 |            256 | 0.0285125 |          128 | 0.363236  |        0.913867 |                   10 |\n",
        "| train_mnist_tune_checkpoint_85489_00009 | TERMINATED |       |             32 |            256 | 0.001     |           64 | 0.150946  |        0.964201 |                   10 |\n",
        "+-----------------------------------------+------------+-------+----------------+----------------+-----------+--------------+-----------+-----------------+----------------------+\n",
        "```\n",
        "\n",
        "As you can see, each sample ran the full number of 10 iterations.\n",
        "All trials ended with quite good parameter combinations and showed relatively good performances.\n",
        "In some runs, the parameters have been perturbed. And the best configuration even reached a\n",
        "mean validation accuracy of `0.987062`!\n",
        "\n",
        "In summary, PyTorch Lightning Modules are easy to extend to use with Tune. It just took\n",
        "us importing one or two callbacks and a small wrapper function to get great performing\n",
        "parameter configurations.\n",
        "\n",
        "## More PyTorch Lightning Examples\n",
        "\n",
        "- {doc}`/tune/examples/includes/mnist_ptl_mini`:\n",
        "  A minimal example of using [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
        "  to train a MNIST model. This example utilizes the Ray Tune-provided\n",
        "  {ref}`PyTorch Lightning callbacks <tune-integration-pytorch-lightning>`.\n",
        "  See also {ref}`this tutorial for a full walkthrough <tune-pytorch-lightning-ref>`.\n",
        "- {ref}`A walkthrough tutorial for using Ray Tune with Pytorch-Lightning <tune-pytorch-lightning-ref>`.\n",
        "- {doc}`/tune/examples/includes/mlflow_ptl_example`: Example for using [MLflow](https://github.com/mlflow/mlflow/)\n",
        "  and [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) with Ray Tune."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orphan": true,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}